{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1co9QUId36Eo"
      },
      "source": [
        "# CSS324 Homework Assignment\n",
        "\n",
        "CIFAR10 is a small image classification dataset. Its objective is to classification an 32x32 color image into 10 classes.\n",
        "\n",
        "See https://www.cs.toronto.edu/~kriz/cifar.html and https://keras.io/api/datasets/cifar10/ for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3zXtrgRH9oK5"
      },
      "outputs": [],
      "source": [
        "# By Narathee Bunpanya 6222780767 & Saritpong Hengsamrithiphol 6222781666\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZMSl6MYp33eW"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vwoz3JDK98Jy",
        "outputId": "606db11d-2cd2-44d7-8298-6b742ede1486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot a training example\n",
        "x = x_train[11, :, :, :]\n",
        "y = y_train[11][0]\n",
        "\n",
        "plt.imshow(x)\n",
        "print(y)        # 7 = horse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVd1ssKvpIGp",
        "outputId": "f82c2206-ca19-4b39-c5eb-769981efea93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdoQO7CiqM4j",
        "outputId": "29f5a3f3-a16b-4cb3-ed12-2668b13e404d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9], dtype=uint8)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFt4OqeP_Buc"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Construct a deep neural network containing three hidden layer to classify images in the CIFAR10 dataset. You can choose the numbers of hidden nodes in three layers, appropriate activation functions, regularizers. Use 20% of the training set to validate the model.\n",
        "\n",
        "After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kpuD33bU__mt"
      },
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "\n",
        "# Normalize input vectors\n",
        "# [0, 255] --> [0.0, 1.0]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# # Encode target outputs using one-hot encoding\n",
        "\n",
        "# y_train = tf.one_hot(y_train, 10)\n",
        "# y_test = tf.one_hot(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn3mmOWWpYTd",
        "outputId": "47e62884-cebd-4711-90fc-8c33b9052f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) (50000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AJDMKbqgpOa",
        "outputId": "d1d08c9e-2b39-4322-8ef6-9235da53f859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 3072)             12288     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               393344    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 415,306\n",
            "Trainable params: 408,778\n",
            "Non-trainable params: 6,528\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define a model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(32, 32,3)),\n",
        "   \n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu',input_shape=(30,),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    \n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\t\t\t\n",
        "    tf.keras.layers.Dense(64, activation='relu',input_shape=(30,),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\t\t\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid',input_shape=(30,))\n",
        "    \n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aIQZIYVkGhs",
        "outputId": "e5e77156-683f-4633-daab-abce6ce368f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "40/40 [==============================] - 5s 29ms/step - loss: 2.2719 - accuracy: 0.2623 - val_loss: 2.0080 - val_accuracy: 0.2732\n",
            "Epoch 2/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.9542 - accuracy: 0.3318 - val_loss: 1.8678 - val_accuracy: 0.3554\n",
            "Epoch 3/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.8478 - accuracy: 0.3623 - val_loss: 1.8047 - val_accuracy: 0.3907\n",
            "Epoch 4/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.7712 - accuracy: 0.3836 - val_loss: 1.7766 - val_accuracy: 0.4009\n",
            "Epoch 5/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.7234 - accuracy: 0.4018 - val_loss: 1.7291 - val_accuracy: 0.4079\n",
            "Epoch 6/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.6856 - accuracy: 0.4129 - val_loss: 1.6642 - val_accuracy: 0.4322\n",
            "Epoch 7/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.6519 - accuracy: 0.4244 - val_loss: 1.6206 - val_accuracy: 0.4428\n",
            "Epoch 8/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.6298 - accuracy: 0.4322 - val_loss: 1.5742 - val_accuracy: 0.4615\n",
            "Epoch 9/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.6010 - accuracy: 0.4443 - val_loss: 1.5410 - val_accuracy: 0.4722\n",
            "Epoch 10/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5836 - accuracy: 0.4483 - val_loss: 1.5160 - val_accuracy: 0.4776\n",
            "Epoch 11/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5658 - accuracy: 0.4570 - val_loss: 1.4988 - val_accuracy: 0.4833\n",
            "Epoch 12/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5461 - accuracy: 0.4617 - val_loss: 1.4825 - val_accuracy: 0.4875\n",
            "Epoch 13/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5391 - accuracy: 0.4654 - val_loss: 1.4685 - val_accuracy: 0.4907\n",
            "Epoch 14/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5283 - accuracy: 0.4703 - val_loss: 1.4553 - val_accuracy: 0.4958\n",
            "Epoch 15/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5189 - accuracy: 0.4738 - val_loss: 1.4487 - val_accuracy: 0.5032\n",
            "Epoch 16/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.5077 - accuracy: 0.4755 - val_loss: 1.4430 - val_accuracy: 0.5037\n",
            "Epoch 17/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4925 - accuracy: 0.4830 - val_loss: 1.4307 - val_accuracy: 0.5035\n",
            "Epoch 18/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4818 - accuracy: 0.4836 - val_loss: 1.4326 - val_accuracy: 0.5064\n",
            "Epoch 19/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4748 - accuracy: 0.4872 - val_loss: 1.4209 - val_accuracy: 0.5091\n",
            "Epoch 20/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4636 - accuracy: 0.4942 - val_loss: 1.4128 - val_accuracy: 0.5144\n",
            "Epoch 21/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4520 - accuracy: 0.4968 - val_loss: 1.4146 - val_accuracy: 0.5136\n",
            "Epoch 22/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4499 - accuracy: 0.4962 - val_loss: 1.4074 - val_accuracy: 0.5186\n",
            "Epoch 23/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4415 - accuracy: 0.5041 - val_loss: 1.3994 - val_accuracy: 0.5248\n",
            "Epoch 24/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4342 - accuracy: 0.5070 - val_loss: 1.3966 - val_accuracy: 0.5177\n",
            "Epoch 25/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4324 - accuracy: 0.5048 - val_loss: 1.4080 - val_accuracy: 0.5140\n",
            "Epoch 26/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4237 - accuracy: 0.5082 - val_loss: 1.3956 - val_accuracy: 0.5178\n",
            "Epoch 27/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.4177 - accuracy: 0.5097 - val_loss: 1.3864 - val_accuracy: 0.5246\n",
            "Epoch 28/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4066 - accuracy: 0.5128 - val_loss: 1.3956 - val_accuracy: 0.5234\n",
            "Epoch 29/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.4087 - accuracy: 0.5135 - val_loss: 1.3808 - val_accuracy: 0.5269\n",
            "Epoch 30/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.4032 - accuracy: 0.5167 - val_loss: 1.3814 - val_accuracy: 0.5310\n",
            "Epoch 31/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3997 - accuracy: 0.5177 - val_loss: 1.3835 - val_accuracy: 0.5231\n",
            "Epoch 32/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3926 - accuracy: 0.5197 - val_loss: 1.3770 - val_accuracy: 0.5303\n",
            "Epoch 33/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3852 - accuracy: 0.5221 - val_loss: 1.3659 - val_accuracy: 0.5311\n",
            "Epoch 34/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3726 - accuracy: 0.5274 - val_loss: 1.3740 - val_accuracy: 0.5313\n",
            "Epoch 35/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3856 - accuracy: 0.5247 - val_loss: 1.3701 - val_accuracy: 0.5325\n",
            "Epoch 36/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3682 - accuracy: 0.5307 - val_loss: 1.3610 - val_accuracy: 0.5381\n",
            "Epoch 37/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3730 - accuracy: 0.5282 - val_loss: 1.3683 - val_accuracy: 0.5328\n",
            "Epoch 38/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3638 - accuracy: 0.5326 - val_loss: 1.3647 - val_accuracy: 0.5315\n",
            "Epoch 39/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3571 - accuracy: 0.5323 - val_loss: 1.3680 - val_accuracy: 0.5335\n",
            "Epoch 40/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3585 - accuracy: 0.5342 - val_loss: 1.3620 - val_accuracy: 0.5339\n",
            "Epoch 41/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3632 - accuracy: 0.5338 - val_loss: 1.3683 - val_accuracy: 0.5327\n",
            "Epoch 42/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3547 - accuracy: 0.5367 - val_loss: 1.3675 - val_accuracy: 0.5338\n",
            "Epoch 43/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3503 - accuracy: 0.5390 - val_loss: 1.3623 - val_accuracy: 0.5336\n",
            "Epoch 44/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3512 - accuracy: 0.5389 - val_loss: 1.3607 - val_accuracy: 0.5371\n",
            "Epoch 45/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3495 - accuracy: 0.5408 - val_loss: 1.3592 - val_accuracy: 0.5397\n",
            "Epoch 46/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3410 - accuracy: 0.5426 - val_loss: 1.3513 - val_accuracy: 0.5406\n",
            "Epoch 47/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3460 - accuracy: 0.5412 - val_loss: 1.3592 - val_accuracy: 0.5379\n",
            "Epoch 48/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3424 - accuracy: 0.5433 - val_loss: 1.3592 - val_accuracy: 0.5384\n",
            "Epoch 49/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3317 - accuracy: 0.5465 - val_loss: 1.3572 - val_accuracy: 0.5365\n",
            "Epoch 50/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3325 - accuracy: 0.5433 - val_loss: 1.3537 - val_accuracy: 0.5429\n",
            "Epoch 51/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3254 - accuracy: 0.5496 - val_loss: 1.3582 - val_accuracy: 0.5422\n",
            "Epoch 52/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3272 - accuracy: 0.5484 - val_loss: 1.3529 - val_accuracy: 0.5423\n",
            "Epoch 53/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3232 - accuracy: 0.5522 - val_loss: 1.3564 - val_accuracy: 0.5391\n",
            "Epoch 54/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3142 - accuracy: 0.5527 - val_loss: 1.3546 - val_accuracy: 0.5398\n",
            "Epoch 55/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3236 - accuracy: 0.5497 - val_loss: 1.3567 - val_accuracy: 0.5388\n",
            "Epoch 56/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3111 - accuracy: 0.5541 - val_loss: 1.3524 - val_accuracy: 0.5432\n",
            "Epoch 57/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3164 - accuracy: 0.5499 - val_loss: 1.3558 - val_accuracy: 0.5454\n",
            "Epoch 58/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3092 - accuracy: 0.5561 - val_loss: 1.3550 - val_accuracy: 0.5396\n",
            "Epoch 59/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3198 - accuracy: 0.5508 - val_loss: 1.3541 - val_accuracy: 0.5443\n",
            "Epoch 60/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3099 - accuracy: 0.5561 - val_loss: 1.3632 - val_accuracy: 0.5429\n",
            "Epoch 61/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3090 - accuracy: 0.5557 - val_loss: 1.3601 - val_accuracy: 0.5422\n",
            "Epoch 62/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3021 - accuracy: 0.5589 - val_loss: 1.3534 - val_accuracy: 0.5446\n",
            "Epoch 63/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3056 - accuracy: 0.5562 - val_loss: 1.3546 - val_accuracy: 0.5416\n",
            "Epoch 64/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3041 - accuracy: 0.5576 - val_loss: 1.3558 - val_accuracy: 0.5445\n",
            "Epoch 65/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.3023 - accuracy: 0.5586 - val_loss: 1.3527 - val_accuracy: 0.5438\n",
            "Epoch 66/200\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 1.2911 - accuracy: 0.5634 - val_loss: 1.3519 - val_accuracy: 0.5454\n",
            "Epoch 67/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2939 - accuracy: 0.5614 - val_loss: 1.3535 - val_accuracy: 0.5476\n",
            "Epoch 68/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.3000 - accuracy: 0.5600 - val_loss: 1.3551 - val_accuracy: 0.5451\n",
            "Epoch 69/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.3021 - accuracy: 0.5586 - val_loss: 1.3645 - val_accuracy: 0.5413\n",
            "Epoch 70/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2963 - accuracy: 0.5615 - val_loss: 1.3519 - val_accuracy: 0.5465\n",
            "Epoch 71/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2964 - accuracy: 0.5625 - val_loss: 1.3547 - val_accuracy: 0.5445\n",
            "Epoch 72/200\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 1.2958 - accuracy: 0.5664 - val_loss: 1.3502 - val_accuracy: 0.5467\n",
            "Epoch 73/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2839 - accuracy: 0.5683 - val_loss: 1.3538 - val_accuracy: 0.5453\n",
            "Epoch 74/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2869 - accuracy: 0.5670 - val_loss: 1.3537 - val_accuracy: 0.5443\n",
            "Epoch 75/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2877 - accuracy: 0.5635 - val_loss: 1.3504 - val_accuracy: 0.5496\n",
            "Epoch 76/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2868 - accuracy: 0.5652 - val_loss: 1.3579 - val_accuracy: 0.5478\n",
            "Epoch 77/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2914 - accuracy: 0.5660 - val_loss: 1.3559 - val_accuracy: 0.5481\n",
            "Epoch 78/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2886 - accuracy: 0.5643 - val_loss: 1.3510 - val_accuracy: 0.5531\n",
            "Epoch 79/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2810 - accuracy: 0.5692 - val_loss: 1.3578 - val_accuracy: 0.5465\n",
            "Epoch 80/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2903 - accuracy: 0.5678 - val_loss: 1.3493 - val_accuracy: 0.5525\n",
            "Epoch 81/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2833 - accuracy: 0.5695 - val_loss: 1.3517 - val_accuracy: 0.5471\n",
            "Epoch 82/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2860 - accuracy: 0.5700 - val_loss: 1.3585 - val_accuracy: 0.5453\n",
            "Epoch 83/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2885 - accuracy: 0.5670 - val_loss: 1.3554 - val_accuracy: 0.5469\n",
            "Epoch 84/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2838 - accuracy: 0.5696 - val_loss: 1.3543 - val_accuracy: 0.5493\n",
            "Epoch 85/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2902 - accuracy: 0.5664 - val_loss: 1.3563 - val_accuracy: 0.5488\n",
            "Epoch 86/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2861 - accuracy: 0.5680 - val_loss: 1.3522 - val_accuracy: 0.5520\n",
            "Epoch 87/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2827 - accuracy: 0.5699 - val_loss: 1.3513 - val_accuracy: 0.5495\n",
            "Epoch 88/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2775 - accuracy: 0.5696 - val_loss: 1.3481 - val_accuracy: 0.5511\n",
            "Epoch 89/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2719 - accuracy: 0.5745 - val_loss: 1.3543 - val_accuracy: 0.5523\n",
            "Epoch 90/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2780 - accuracy: 0.5742 - val_loss: 1.3566 - val_accuracy: 0.5468\n",
            "Epoch 91/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2715 - accuracy: 0.5704 - val_loss: 1.3577 - val_accuracy: 0.5471\n",
            "Epoch 92/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2750 - accuracy: 0.5755 - val_loss: 1.3591 - val_accuracy: 0.5532\n",
            "Epoch 93/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2763 - accuracy: 0.5734 - val_loss: 1.3554 - val_accuracy: 0.5515\n",
            "Epoch 94/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2684 - accuracy: 0.5757 - val_loss: 1.3587 - val_accuracy: 0.5465\n",
            "Epoch 95/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2759 - accuracy: 0.5716 - val_loss: 1.3593 - val_accuracy: 0.5467\n",
            "Epoch 96/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2811 - accuracy: 0.5706 - val_loss: 1.3598 - val_accuracy: 0.5429\n",
            "Epoch 97/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2773 - accuracy: 0.5720 - val_loss: 1.3667 - val_accuracy: 0.5495\n",
            "Epoch 98/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2787 - accuracy: 0.5725 - val_loss: 1.3634 - val_accuracy: 0.5493\n",
            "Epoch 99/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2738 - accuracy: 0.5748 - val_loss: 1.3513 - val_accuracy: 0.5547\n",
            "Epoch 100/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2752 - accuracy: 0.5723 - val_loss: 1.3534 - val_accuracy: 0.5487\n",
            "Epoch 101/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2665 - accuracy: 0.5755 - val_loss: 1.3533 - val_accuracy: 0.5532\n",
            "Epoch 102/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2644 - accuracy: 0.5764 - val_loss: 1.3594 - val_accuracy: 0.5453\n",
            "Epoch 103/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2684 - accuracy: 0.5765 - val_loss: 1.3577 - val_accuracy: 0.5502\n",
            "Epoch 104/200\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 1.2694 - accuracy: 0.5754 - val_loss: 1.3601 - val_accuracy: 0.5477\n",
            "Epoch 105/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2620 - accuracy: 0.5793 - val_loss: 1.3521 - val_accuracy: 0.5502\n",
            "Epoch 106/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2658 - accuracy: 0.5743 - val_loss: 1.3524 - val_accuracy: 0.5510\n",
            "Epoch 107/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2675 - accuracy: 0.5793 - val_loss: 1.3601 - val_accuracy: 0.5486\n",
            "Epoch 108/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2624 - accuracy: 0.5819 - val_loss: 1.3536 - val_accuracy: 0.5516\n",
            "Epoch 109/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2668 - accuracy: 0.5778 - val_loss: 1.3522 - val_accuracy: 0.5487\n",
            "Epoch 110/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2630 - accuracy: 0.5821 - val_loss: 1.3545 - val_accuracy: 0.5518\n",
            "Epoch 111/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2573 - accuracy: 0.5782 - val_loss: 1.3633 - val_accuracy: 0.5464\n",
            "Epoch 112/200\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 1.2619 - accuracy: 0.5807 - val_loss: 1.3598 - val_accuracy: 0.5478\n",
            "Epoch 113/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2571 - accuracy: 0.5818 - val_loss: 1.3604 - val_accuracy: 0.5449\n",
            "Epoch 114/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2508 - accuracy: 0.5845 - val_loss: 1.3583 - val_accuracy: 0.5545\n",
            "Epoch 115/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2601 - accuracy: 0.5812 - val_loss: 1.3545 - val_accuracy: 0.5476\n",
            "Epoch 116/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2550 - accuracy: 0.5852 - val_loss: 1.3601 - val_accuracy: 0.5486\n",
            "Epoch 117/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2513 - accuracy: 0.5831 - val_loss: 1.3621 - val_accuracy: 0.5466\n",
            "Epoch 118/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2594 - accuracy: 0.5820 - val_loss: 1.3684 - val_accuracy: 0.5486\n",
            "Epoch 119/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2592 - accuracy: 0.5820 - val_loss: 1.3619 - val_accuracy: 0.5500\n",
            "Epoch 120/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2509 - accuracy: 0.5848 - val_loss: 1.3592 - val_accuracy: 0.5497\n",
            "Epoch 121/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2471 - accuracy: 0.5865 - val_loss: 1.3659 - val_accuracy: 0.5489\n",
            "Epoch 122/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2510 - accuracy: 0.5828 - val_loss: 1.3588 - val_accuracy: 0.5514\n",
            "Epoch 123/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2575 - accuracy: 0.5809 - val_loss: 1.3679 - val_accuracy: 0.5463\n",
            "Epoch 124/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2573 - accuracy: 0.5846 - val_loss: 1.3638 - val_accuracy: 0.5501\n",
            "Epoch 125/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2533 - accuracy: 0.5843 - val_loss: 1.3607 - val_accuracy: 0.5516\n",
            "Epoch 126/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2595 - accuracy: 0.5845 - val_loss: 1.3598 - val_accuracy: 0.5476\n",
            "Epoch 127/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2621 - accuracy: 0.5817 - val_loss: 1.3643 - val_accuracy: 0.5479\n",
            "Epoch 128/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2542 - accuracy: 0.5814 - val_loss: 1.3659 - val_accuracy: 0.5511\n",
            "Epoch 129/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2498 - accuracy: 0.5870 - val_loss: 1.3653 - val_accuracy: 0.5481\n",
            "Epoch 130/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2492 - accuracy: 0.5836 - val_loss: 1.3678 - val_accuracy: 0.5499\n",
            "Epoch 131/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2464 - accuracy: 0.5863 - val_loss: 1.3614 - val_accuracy: 0.5527\n",
            "Epoch 132/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2497 - accuracy: 0.5845 - val_loss: 1.3638 - val_accuracy: 0.5491\n",
            "Epoch 133/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2530 - accuracy: 0.5849 - val_loss: 1.3651 - val_accuracy: 0.5505\n",
            "Epoch 134/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2506 - accuracy: 0.5863 - val_loss: 1.3625 - val_accuracy: 0.5477\n",
            "Epoch 135/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2535 - accuracy: 0.5874 - val_loss: 1.3663 - val_accuracy: 0.5502\n",
            "Epoch 136/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2470 - accuracy: 0.5867 - val_loss: 1.3576 - val_accuracy: 0.5542\n",
            "Epoch 137/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2479 - accuracy: 0.5867 - val_loss: 1.3606 - val_accuracy: 0.5505\n",
            "Epoch 138/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2502 - accuracy: 0.5831 - val_loss: 1.3560 - val_accuracy: 0.5524\n",
            "Epoch 139/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2556 - accuracy: 0.5857 - val_loss: 1.3569 - val_accuracy: 0.5537\n",
            "Epoch 140/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2522 - accuracy: 0.5863 - val_loss: 1.3633 - val_accuracy: 0.5510\n",
            "Epoch 141/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2575 - accuracy: 0.5814 - val_loss: 1.3563 - val_accuracy: 0.5518\n",
            "Epoch 142/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2511 - accuracy: 0.5875 - val_loss: 1.3694 - val_accuracy: 0.5466\n",
            "Epoch 143/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2602 - accuracy: 0.5825 - val_loss: 1.3684 - val_accuracy: 0.5520\n",
            "Epoch 144/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2590 - accuracy: 0.5851 - val_loss: 1.3668 - val_accuracy: 0.5523\n",
            "Epoch 145/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2432 - accuracy: 0.5860 - val_loss: 1.3687 - val_accuracy: 0.5498\n",
            "Epoch 146/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2427 - accuracy: 0.5888 - val_loss: 1.3656 - val_accuracy: 0.5499\n",
            "Epoch 147/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2560 - accuracy: 0.5830 - val_loss: 1.3653 - val_accuracy: 0.5503\n",
            "Epoch 148/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2465 - accuracy: 0.5853 - val_loss: 1.3712 - val_accuracy: 0.5500\n",
            "Epoch 149/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2627 - accuracy: 0.5845 - val_loss: 1.3659 - val_accuracy: 0.5527\n",
            "Epoch 150/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2538 - accuracy: 0.5843 - val_loss: 1.3680 - val_accuracy: 0.5508\n",
            "Epoch 151/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2525 - accuracy: 0.5851 - val_loss: 1.3698 - val_accuracy: 0.5515\n",
            "Epoch 152/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2501 - accuracy: 0.5879 - val_loss: 1.3612 - val_accuracy: 0.5548\n",
            "Epoch 153/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2462 - accuracy: 0.5890 - val_loss: 1.3644 - val_accuracy: 0.5529\n",
            "Epoch 154/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2437 - accuracy: 0.5903 - val_loss: 1.3650 - val_accuracy: 0.5496\n",
            "Epoch 155/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2483 - accuracy: 0.5872 - val_loss: 1.3654 - val_accuracy: 0.5543\n",
            "Epoch 156/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2429 - accuracy: 0.5915 - val_loss: 1.3702 - val_accuracy: 0.5493\n",
            "Epoch 157/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2399 - accuracy: 0.5896 - val_loss: 1.3619 - val_accuracy: 0.5555\n",
            "Epoch 158/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2472 - accuracy: 0.5884 - val_loss: 1.3594 - val_accuracy: 0.5512\n",
            "Epoch 159/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2381 - accuracy: 0.5915 - val_loss: 1.3634 - val_accuracy: 0.5514\n",
            "Epoch 160/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2475 - accuracy: 0.5891 - val_loss: 1.3602 - val_accuracy: 0.5518\n",
            "Epoch 161/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2442 - accuracy: 0.5889 - val_loss: 1.3633 - val_accuracy: 0.5546\n",
            "Epoch 162/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2540 - accuracy: 0.5885 - val_loss: 1.3737 - val_accuracy: 0.5502\n",
            "Epoch 163/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2480 - accuracy: 0.5847 - val_loss: 1.3676 - val_accuracy: 0.5510\n",
            "Epoch 164/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2376 - accuracy: 0.5914 - val_loss: 1.3657 - val_accuracy: 0.5562\n",
            "Epoch 165/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2454 - accuracy: 0.5908 - val_loss: 1.3662 - val_accuracy: 0.5530\n",
            "Epoch 166/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2548 - accuracy: 0.5839 - val_loss: 1.3670 - val_accuracy: 0.5537\n",
            "Epoch 167/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2564 - accuracy: 0.5845 - val_loss: 1.3714 - val_accuracy: 0.5512\n",
            "Epoch 168/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2525 - accuracy: 0.5855 - val_loss: 1.3642 - val_accuracy: 0.5548\n",
            "Epoch 169/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2403 - accuracy: 0.5925 - val_loss: 1.3667 - val_accuracy: 0.5502\n",
            "Epoch 170/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2515 - accuracy: 0.5869 - val_loss: 1.3665 - val_accuracy: 0.5512\n",
            "Epoch 171/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2561 - accuracy: 0.5877 - val_loss: 1.3702 - val_accuracy: 0.5530\n",
            "Epoch 172/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2498 - accuracy: 0.5897 - val_loss: 1.3700 - val_accuracy: 0.5499\n",
            "Epoch 173/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2419 - accuracy: 0.5907 - val_loss: 1.3659 - val_accuracy: 0.5530\n",
            "Epoch 174/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2458 - accuracy: 0.5893 - val_loss: 1.3772 - val_accuracy: 0.5491\n",
            "Epoch 175/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2422 - accuracy: 0.5881 - val_loss: 1.3655 - val_accuracy: 0.5547\n",
            "Epoch 176/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2415 - accuracy: 0.5901 - val_loss: 1.3636 - val_accuracy: 0.5531\n",
            "Epoch 177/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2372 - accuracy: 0.5911 - val_loss: 1.3713 - val_accuracy: 0.5500\n",
            "Epoch 178/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2411 - accuracy: 0.5891 - val_loss: 1.3749 - val_accuracy: 0.5506\n",
            "Epoch 179/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2468 - accuracy: 0.5878 - val_loss: 1.3734 - val_accuracy: 0.5487\n",
            "Epoch 180/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2380 - accuracy: 0.5915 - val_loss: 1.3690 - val_accuracy: 0.5466\n",
            "Epoch 181/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2503 - accuracy: 0.5874 - val_loss: 1.3676 - val_accuracy: 0.5515\n",
            "Epoch 182/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2454 - accuracy: 0.5903 - val_loss: 1.3696 - val_accuracy: 0.5497\n",
            "Epoch 183/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2481 - accuracy: 0.5906 - val_loss: 1.3623 - val_accuracy: 0.5548\n",
            "Epoch 184/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2341 - accuracy: 0.5928 - val_loss: 1.3628 - val_accuracy: 0.5560\n",
            "Epoch 185/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2328 - accuracy: 0.5943 - val_loss: 1.3656 - val_accuracy: 0.5559\n",
            "Epoch 186/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2405 - accuracy: 0.5915 - val_loss: 1.3660 - val_accuracy: 0.5525\n",
            "Epoch 187/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2416 - accuracy: 0.5918 - val_loss: 1.3729 - val_accuracy: 0.5525\n",
            "Epoch 188/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2357 - accuracy: 0.5944 - val_loss: 1.3656 - val_accuracy: 0.5545\n",
            "Epoch 189/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2320 - accuracy: 0.5963 - val_loss: 1.3642 - val_accuracy: 0.5541\n",
            "Epoch 190/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2436 - accuracy: 0.5919 - val_loss: 1.3604 - val_accuracy: 0.5576\n",
            "Epoch 191/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2328 - accuracy: 0.5932 - val_loss: 1.3596 - val_accuracy: 0.5599\n",
            "Epoch 192/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2318 - accuracy: 0.5944 - val_loss: 1.3648 - val_accuracy: 0.5575\n",
            "Epoch 193/200\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 1.2384 - accuracy: 0.5939 - val_loss: 1.3682 - val_accuracy: 0.5518\n",
            "Epoch 194/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2338 - accuracy: 0.5955 - val_loss: 1.3676 - val_accuracy: 0.5572\n",
            "Epoch 195/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2348 - accuracy: 0.5938 - val_loss: 1.3686 - val_accuracy: 0.5560\n",
            "Epoch 196/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2323 - accuracy: 0.5946 - val_loss: 1.3632 - val_accuracy: 0.5577\n",
            "Epoch 197/200\n",
            "40/40 [==============================] - 1s 21ms/step - loss: 1.2448 - accuracy: 0.5899 - val_loss: 1.3684 - val_accuracy: 0.5555\n",
            "Epoch 198/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2365 - accuracy: 0.5923 - val_loss: 1.3664 - val_accuracy: 0.5525\n",
            "Epoch 199/200\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 1.2388 - accuracy: 0.5931 - val_loss: 1.3640 - val_accuracy: 0.5591\n",
            "Epoch 200/200\n",
            "40/40 [==============================] - 1s 19ms/step - loss: 1.2338 - accuracy: 0.5933 - val_loss: 1.3708 - val_accuracy: 0.5539\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=200, batch_size=1024, shuffle=True, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "3Amk91Chjzqz",
        "outputId": "8ce081b2-78b1-40db-de61-8af62e6378c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': [2.271937608718872, 1.9542471170425415, 1.8478233814239502, 1.7711906433105469, 1.7234185934066772, 1.6856046915054321, 1.6519060134887695, 1.6298269033432007, 1.601001501083374, 1.583608627319336, 1.5658254623413086, 1.5460822582244873, 1.539094090461731, 1.528282880783081, 1.518883466720581, 1.5077221393585205, 1.4925193786621094, 1.4818177223205566, 1.474765658378601, 1.4635573625564575, 1.4519785642623901, 1.4499304294586182, 1.441504716873169, 1.4342422485351562, 1.432386875152588, 1.4236571788787842, 1.4176506996154785, 1.4065619707107544, 1.4086740016937256, 1.4031822681427002, 1.399652123451233, 1.3926371335983276, 1.3852014541625977, 1.3726447820663452, 1.3855684995651245, 1.3681837320327759, 1.373015284538269, 1.3637645244598389, 1.3570984601974487, 1.3585110902786255, 1.3631625175476074, 1.3546892404556274, 1.3503214120864868, 1.3512393236160278, 1.349475622177124, 1.341045618057251, 1.3460133075714111, 1.342368483543396, 1.33168363571167, 1.3324929475784302, 1.3253772258758545, 1.3271567821502686, 1.3232299089431763, 1.3141943216323853, 1.3236104249954224, 1.3111037015914917, 1.31644606590271, 1.3092491626739502, 1.3197624683380127, 1.309866189956665, 1.3089584112167358, 1.3020893335342407, 1.3055821657180786, 1.3040541410446167, 1.3022552728652954, 1.291063666343689, 1.2939352989196777, 1.3000093698501587, 1.3020565509796143, 1.2962908744812012, 1.2964061498641968, 1.2957760095596313, 1.2839138507843018, 1.2868794202804565, 1.287697434425354, 1.286834955215454, 1.2913875579833984, 1.2886016368865967, 1.2810207605361938, 1.2903385162353516, 1.283326506614685, 1.2859597206115723, 1.2884637117385864, 1.283844232559204, 1.2902042865753174, 1.2860643863677979, 1.2827345132827759, 1.2775089740753174, 1.27191960811615, 1.2779605388641357, 1.2715309858322144, 1.2749968767166138, 1.2762771844863892, 1.2684417963027954, 1.2758679389953613, 1.2810896635055542, 1.2772961854934692, 1.2787392139434814, 1.27377450466156, 1.2751516103744507, 1.2665002346038818, 1.2644104957580566, 1.2684422731399536, 1.2693835496902466, 1.2619719505310059, 1.265826940536499, 1.2674843072891235, 1.2624000310897827, 1.2668169736862183, 1.2630153894424438, 1.2572681903839111, 1.2618722915649414, 1.2570769786834717, 1.2507832050323486, 1.260074257850647, 1.2549569606781006, 1.251291036605835, 1.2594298124313354, 1.2592158317565918, 1.2508907318115234, 1.247136116027832, 1.2509939670562744, 1.257489562034607, 1.2572733163833618, 1.2532705068588257, 1.259514331817627, 1.2620834112167358, 1.254228949546814, 1.2498174905776978, 1.2492421865463257, 1.246443510055542, 1.2496591806411743, 1.25300931930542, 1.2505661249160767, 1.253496766090393, 1.2469518184661865, 1.2479326725006104, 1.2501637935638428, 1.2556402683258057, 1.2521662712097168, 1.2575494050979614, 1.2510710954666138, 1.260213017463684, 1.2589985132217407, 1.243196725845337, 1.2427040338516235, 1.2559678554534912, 1.2464531660079956, 1.2627044916152954, 1.2538254261016846, 1.2525458335876465, 1.250099778175354, 1.2461613416671753, 1.2436802387237549, 1.2483183145523071, 1.2428529262542725, 1.2398829460144043, 1.2471975088119507, 1.238081693649292, 1.2475135326385498, 1.2441816329956055, 1.2539972066879272, 1.2479618787765503, 1.2375586032867432, 1.2454153299331665, 1.2547823190689087, 1.2564018964767456, 1.2524722814559937, 1.2403223514556885, 1.2515147924423218, 1.2561004161834717, 1.2497987747192383, 1.241850733757019, 1.2458429336547852, 1.242173194885254, 1.2414811849594116, 1.2372119426727295, 1.2411147356033325, 1.2467883825302124, 1.2380391359329224, 1.2502599954605103, 1.2454102039337158, 1.2481108903884888, 1.2341198921203613, 1.232772707939148, 1.2405340671539307, 1.2415741682052612, 1.2356650829315186, 1.2319761514663696, 1.2436137199401855, 1.2328312397003174, 1.2317531108856201, 1.238381028175354, 1.2337673902511597, 1.2348167896270752, 1.2322636842727661, 1.2447844743728638, 1.2364786863327026, 1.2387983798980713, 1.2337994575500488], 'accuracy': [0.26225000619888306, 0.3317750096321106, 0.3623499870300293, 0.38362500071525574, 0.4017750024795532, 0.4128749966621399, 0.4243749976158142, 0.4322499930858612, 0.44427499175071716, 0.44827499985694885, 0.4569750130176544, 0.46172499656677246, 0.4653500020503998, 0.4703499972820282, 0.47382500767707825, 0.47554999589920044, 0.48304998874664307, 0.4835500121116638, 0.487199991941452, 0.49422499537467957, 0.4968250095844269, 0.4962249994277954, 0.5040749907493591, 0.5070000290870667, 0.5048249959945679, 0.5082250237464905, 0.5096750259399414, 0.5127750039100647, 0.5134750008583069, 0.5166500210762024, 0.5177000164985657, 0.5197250247001648, 0.5221499800682068, 0.527400016784668, 0.5246999859809875, 0.5307499766349792, 0.528249979019165, 0.5325750112533569, 0.5322999954223633, 0.5341500043869019, 0.5338249802589417, 0.5366500020027161, 0.5389999747276306, 0.5388500094413757, 0.5408499836921692, 0.5426499843597412, 0.5411750078201294, 0.5432999730110168, 0.5465250015258789, 0.5432500243186951, 0.5496000051498413, 0.548425018787384, 0.5522000193595886, 0.5526750087738037, 0.5497000217437744, 0.5541250109672546, 0.5499250292778015, 0.5561249852180481, 0.550849974155426, 0.5561249852180481, 0.5556750297546387, 0.558899998664856, 0.5562499761581421, 0.557574987411499, 0.5586249828338623, 0.5634499788284302, 0.561424970626831, 0.5600000023841858, 0.5585500001907349, 0.5615249872207642, 0.5624750256538391, 0.5663750171661377, 0.5682500004768372, 0.5669749975204468, 0.5634999871253967, 0.5652250051498413, 0.5660499930381775, 0.5642750263214111, 0.5691750049591064, 0.5678250193595886, 0.5695000290870667, 0.5699750185012817, 0.5670499801635742, 0.5695750117301941, 0.5664499998092651, 0.5680000185966492, 0.5699499845504761, 0.5695750117301941, 0.5745499730110168, 0.5742250084877014, 0.5703750252723694, 0.5754749774932861, 0.5734000205993652, 0.5756999850273132, 0.5716000199317932, 0.5705999732017517, 0.5720250010490417, 0.5724999904632568, 0.574774980545044, 0.5723249912261963, 0.5754500031471252, 0.576449990272522, 0.5764999985694885, 0.5753999948501587, 0.5793499946594238, 0.5743499994277954, 0.5792750120162964, 0.5818750262260437, 0.5778250098228455, 0.5820500254631042, 0.5781750082969666, 0.5806999802589417, 0.581849992275238, 0.5844749808311462, 0.5812249779701233, 0.5852000117301941, 0.583050012588501, 0.5819500088691711, 0.581974983215332, 0.5848000049591064, 0.5864750146865845, 0.5827749967575073, 0.5809000134468079, 0.5845749974250793, 0.5842750072479248, 0.5845000147819519, 0.5816500186920166, 0.5813999772071838, 0.5869749784469604, 0.5836250185966492, 0.5862749814987183, 0.5844749808311462, 0.584850013256073, 0.5863000154495239, 0.5873500108718872, 0.5867000222206116, 0.5866749882698059, 0.583050012588501, 0.5856750011444092, 0.5863000154495239, 0.5813500285148621, 0.5875499844551086, 0.5825499892234802, 0.585099995136261, 0.5859500169754028, 0.588824987411499, 0.5829749703407288, 0.5853250026702881, 0.5845249891281128, 0.5842750072479248, 0.5850750207901001, 0.5879499912261963, 0.5890250205993652, 0.5903249979019165, 0.5871750116348267, 0.5915499925613403, 0.5896250009536743, 0.5884000062942505, 0.5915250182151794, 0.5891249775886536, 0.5888500213623047, 0.5885249972343445, 0.584725022315979, 0.5914000272750854, 0.5907750129699707, 0.5839250087738037, 0.5844500064849854, 0.5855000019073486, 0.5925250053405762, 0.586899995803833, 0.5876500010490417, 0.5896750092506409, 0.5906500220298767, 0.5892500281333923, 0.5880749821662903, 0.5900750160217285, 0.5910999774932861, 0.5891000032424927, 0.5878499746322632, 0.5914999842643738, 0.5873749852180481, 0.5903499722480774, 0.5905749797821045, 0.5927500128746033, 0.5943250060081482, 0.5915499925613403, 0.5917999744415283, 0.5944499969482422, 0.5962749719619751, 0.5919250249862671, 0.5932499766349792, 0.5944250226020813, 0.5939249992370605, 0.5955250263214111, 0.5938249826431274, 0.5945500135421753, 0.5898500084877014, 0.5922999978065491, 0.5930749773979187, 0.5933499932289124], 'val_loss': [2.0080151557922363, 1.8678027391433716, 1.804653525352478, 1.7765570878982544, 1.729143738746643, 1.6641793251037598, 1.6206004619598389, 1.5742120742797852, 1.5409528017044067, 1.5159757137298584, 1.49879789352417, 1.482532024383545, 1.4684736728668213, 1.4552522897720337, 1.4486502408981323, 1.4429724216461182, 1.43070387840271, 1.4326112270355225, 1.4209208488464355, 1.4128026962280273, 1.4145731925964355, 1.4074169397354126, 1.3994120359420776, 1.3966387510299683, 1.4080321788787842, 1.3955990076065063, 1.3863822221755981, 1.3955893516540527, 1.380822777748108, 1.3814030885696411, 1.3834573030471802, 1.3770076036453247, 1.365889310836792, 1.3740376234054565, 1.370113730430603, 1.3609868288040161, 1.3682630062103271, 1.3647243976593018, 1.367994785308838, 1.361999750137329, 1.3682767152786255, 1.3674999475479126, 1.3622877597808838, 1.3607209920883179, 1.3591850996017456, 1.3512609004974365, 1.359197974205017, 1.3591572046279907, 1.3571912050247192, 1.3537050485610962, 1.3582353591918945, 1.3528525829315186, 1.3563742637634277, 1.3546302318572998, 1.3567193746566772, 1.3524147272109985, 1.3558460474014282, 1.3549548387527466, 1.3541456460952759, 1.3631913661956787, 1.360052227973938, 1.35337495803833, 1.354552149772644, 1.355816125869751, 1.3526577949523926, 1.3519306182861328, 1.3534834384918213, 1.3550984859466553, 1.3644979000091553, 1.3519030809402466, 1.3547435998916626, 1.3502247333526611, 1.3537575006484985, 1.3537384271621704, 1.3504241704940796, 1.3578912019729614, 1.3558719158172607, 1.3510254621505737, 1.3578418493270874, 1.3492913246154785, 1.3516768217086792, 1.3584861755371094, 1.3554446697235107, 1.3542548418045044, 1.3562780618667603, 1.3521591424942017, 1.3513456583023071, 1.3480985164642334, 1.3542888164520264, 1.356582760810852, 1.3576816320419312, 1.3590950965881348, 1.3554322719573975, 1.358734130859375, 1.3592725992202759, 1.35982084274292, 1.3666821718215942, 1.363427758216858, 1.3512659072875977, 1.3533962965011597, 1.3533324003219604, 1.3594107627868652, 1.3576858043670654, 1.3601337671279907, 1.3521299362182617, 1.3524489402770996, 1.360077977180481, 1.353647232055664, 1.352221965789795, 1.3544819355010986, 1.363284707069397, 1.3597755432128906, 1.3603801727294922, 1.3583463430404663, 1.3544745445251465, 1.360140085220337, 1.3620740175247192, 1.3683843612670898, 1.3618836402893066, 1.3591969013214111, 1.3658671379089355, 1.3587547540664673, 1.3679307699203491, 1.3637551069259644, 1.3607203960418701, 1.3597792387008667, 1.3643097877502441, 1.365919589996338, 1.3653393983840942, 1.3677929639816284, 1.3613865375518799, 1.3638004064559937, 1.365110993385315, 1.362523078918457, 1.3663346767425537, 1.3575587272644043, 1.360621452331543, 1.3559540510177612, 1.356862187385559, 1.3633044958114624, 1.3563240766525269, 1.369367241859436, 1.3684414625167847, 1.3668235540390015, 1.3687207698822021, 1.365554928779602, 1.3652864694595337, 1.3711812496185303, 1.365887999534607, 1.36798095703125, 1.3697681427001953, 1.3611562252044678, 1.364381194114685, 1.364997148513794, 1.3653690814971924, 1.3701847791671753, 1.3619155883789062, 1.359354853630066, 1.3634120225906372, 1.3601635694503784, 1.363268494606018, 1.3736990690231323, 1.3675832748413086, 1.3657019138336182, 1.366212010383606, 1.367038369178772, 1.3714196681976318, 1.3642336130142212, 1.366682767868042, 1.366516351699829, 1.370176911354065, 1.3699915409088135, 1.3658783435821533, 1.3772079944610596, 1.3654628992080688, 1.3635823726654053, 1.371301531791687, 1.3748574256896973, 1.3734312057495117, 1.3689535856246948, 1.3675960302352905, 1.3695675134658813, 1.362331509590149, 1.3628441095352173, 1.3655812740325928, 1.365972876548767, 1.3729482889175415, 1.365583062171936, 1.364227056503296, 1.3604099750518799, 1.3596328496932983, 1.364846110343933, 1.36819589138031, 1.3675744533538818, 1.3685572147369385, 1.36320161819458, 1.3684179782867432, 1.3664227724075317, 1.363968014717102, 1.3707948923110962], 'val_accuracy': [0.27320000529289246, 0.3553999960422516, 0.39070001244544983, 0.4009000062942505, 0.40790000557899475, 0.43220001459121704, 0.44279998540878296, 0.46149998903274536, 0.4722000062465668, 0.47760000824928284, 0.48330000042915344, 0.48750001192092896, 0.49070000648498535, 0.4957999885082245, 0.5031999945640564, 0.5037000179290771, 0.5034999847412109, 0.5063999891281128, 0.5091000199317932, 0.5144000053405762, 0.5135999917984009, 0.5185999870300293, 0.5248000025749207, 0.5177000164985657, 0.5139999985694885, 0.517799973487854, 0.5246000289916992, 0.5234000086784363, 0.5268999934196472, 0.531000018119812, 0.5231000185012817, 0.5303000211715698, 0.5310999751091003, 0.5313000082969666, 0.5325000286102295, 0.538100004196167, 0.532800018787384, 0.531499981880188, 0.5335000157356262, 0.5339000225067139, 0.5327000021934509, 0.5338000059127808, 0.5335999727249146, 0.5371000170707703, 0.5396999716758728, 0.5406000018119812, 0.5378999710083008, 0.5383999943733215, 0.5364999771118164, 0.542900025844574, 0.5422000288963318, 0.5422999858856201, 0.5390999913215637, 0.5397999882698059, 0.5388000011444092, 0.5432000160217285, 0.5454000234603882, 0.5396000146865845, 0.5443000197410583, 0.542900025844574, 0.5422000288963318, 0.5446000099182129, 0.5415999889373779, 0.5444999933242798, 0.5437999963760376, 0.5454000234603882, 0.5475999712944031, 0.5450999736785889, 0.5412999987602234, 0.546500027179718, 0.5444999933242798, 0.5467000007629395, 0.5453000068664551, 0.5443000197410583, 0.5496000051498413, 0.5478000044822693, 0.5480999946594238, 0.5530999898910522, 0.546500027179718, 0.5525000095367432, 0.5471000075340271, 0.5453000068664551, 0.5468999743461609, 0.5493000149726868, 0.548799991607666, 0.5519999861717224, 0.5494999885559082, 0.5511000156402588, 0.552299976348877, 0.5468000173568726, 0.5471000075340271, 0.5532000064849854, 0.5515000224113464, 0.546500027179718, 0.5467000007629395, 0.542900025844574, 0.5494999885559082, 0.5493000149726868, 0.5547000169754028, 0.5486999750137329, 0.5532000064849854, 0.5453000068664551, 0.5501999855041504, 0.5476999878883362, 0.5501999855041504, 0.5509999990463257, 0.5486000180244446, 0.5515999794006348, 0.5486999750137329, 0.551800012588501, 0.5464000105857849, 0.5478000044822693, 0.5449000000953674, 0.5544999837875366, 0.5475999712944031, 0.5486000180244446, 0.5465999841690063, 0.5486000180244446, 0.550000011920929, 0.5497000217437744, 0.5489000082015991, 0.5514000058174133, 0.5462999939918518, 0.5501000285148621, 0.5515999794006348, 0.5475999712944031, 0.5479000210762024, 0.5511000156402588, 0.5480999946594238, 0.5498999953269958, 0.5526999831199646, 0.5490999817848206, 0.5504999756813049, 0.5476999878883362, 0.5501999855041504, 0.5541999936103821, 0.5504999756813049, 0.5523999929428101, 0.5536999702453613, 0.5509999990463257, 0.551800012588501, 0.5465999841690063, 0.5519999861717224, 0.552299976348877, 0.5497999787330627, 0.5498999953269958, 0.5503000020980835, 0.550000011920929, 0.5526999831199646, 0.5508000254631042, 0.5515000224113464, 0.5547999739646912, 0.5529000163078308, 0.5496000051498413, 0.5543000102043152, 0.5493000149726868, 0.5554999709129333, 0.5511999726295471, 0.5514000058174133, 0.551800012588501, 0.5546000003814697, 0.5501999855041504, 0.5509999990463257, 0.5562000274658203, 0.5529999732971191, 0.5536999702453613, 0.5511999726295471, 0.5547999739646912, 0.5501999855041504, 0.5511999726295471, 0.5529999732971191, 0.5498999953269958, 0.5529999732971191, 0.5490999817848206, 0.5547000169754028, 0.5530999898910522, 0.550000011920929, 0.550599992275238, 0.5486999750137329, 0.5465999841690063, 0.5515000224113464, 0.5497000217437744, 0.5547999739646912, 0.5559999942779541, 0.555899977684021, 0.5525000095367432, 0.5525000095367432, 0.5544999837875366, 0.554099977016449, 0.5576000213623047, 0.5598999857902527, 0.5575000047683716, 0.551800012588501, 0.557200014591217, 0.5559999942779541, 0.557699978351593, 0.5554999709129333, 0.5525000095367432, 0.5590999722480774, 0.5539000034332275]}\n",
            "[1.010599970817566, 0.6923999786376953]\n",
            "[1.3642491102218628, 0.5485000014305115]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the training loss and validation loss\n",
        "print(history.history)\n",
        "\n",
        "# Evaluate the model\n",
        "print(model.evaluate(x_train, y_train, verbose=0))\n",
        "print(model.evaluate(x_test, y_test, verbose=0))\n",
        "\n",
        "loss_train = np.array(history.history['loss'])\n",
        "loss_test = np.array(history.history['val_loss'])\n",
        "\n",
        "x = np.arange(0, loss_train.shape[0])\n",
        "plt.plot(x, loss_train, label=\"Training loss\")\n",
        "plt.plot(x, loss_test, label=\"Validation loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fEbEqLfADJu"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Construct a convolutional neural network using your own structure. Try to maximize the prediction accuracy of your model.\n",
        "\n",
        "After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mTrbgdtZknEM"
      },
      "outputs": [],
      "source": [
        "# Your implementation for Question 2\n",
        "#Preprocesses\n",
        "x_train= x_train/255\n",
        "x_test= x_test/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7AXd5jo50sYQ"
      },
      "outputs": [],
      "source": [
        "#Reshape the model\n",
        "x_train = x_train.reshape((-3, 32, 32, 3))\n",
        "x_test = x_test.reshape((-3, 32, 32, 3))\n",
        "tf.random.set_seed(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d_eU1tz0slr",
        "outputId": "e75e0523-445c-45fa-896a-cf6d294c0d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 30, 30, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 30, 30, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 5, 5, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " spatial_dropout2d_2 (Spatia  (None, 5, 5, 128)        0         \n",
            " lDropout2D)                                                     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 3200)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               819456    \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 899,466\n",
            "Trainable params: 898,570\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(32, 32, 3)),\n",
        "    tf.keras.layers.Conv2D(64, (3,3)), \n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D((3,3), strides=(2,2)),\n",
        "    tf.keras.layers.Conv2D(128, (3,3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPooling2D((3,3), strides=(2,2)),\n",
        "    tf.keras.layers.SpatialDropout2D(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\t\t\t\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  \n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpaRWeN-5HJx",
        "outputId": "0b599724-a1f3-40d2-f4b1-ccf1f296c1f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "79/79 [==============================] - 6s 67ms/step - loss: 1.9092 - accuracy: 0.3843 - val_loss: 2.8837 - val_accuracy: 0.0952\n",
            "Epoch 2/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.4701 - accuracy: 0.5039 - val_loss: 3.3147 - val_accuracy: 0.1028\n",
            "Epoch 3/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.3311 - accuracy: 0.5533 - val_loss: 3.1167 - val_accuracy: 0.1059\n",
            "Epoch 4/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.2397 - accuracy: 0.5850 - val_loss: 3.0360 - val_accuracy: 0.1237\n",
            "Epoch 5/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.1812 - accuracy: 0.6077 - val_loss: 2.2792 - val_accuracy: 0.2960\n",
            "Epoch 6/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.1365 - accuracy: 0.6225 - val_loss: 1.9413 - val_accuracy: 0.3623\n",
            "Epoch 7/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.1047 - accuracy: 0.6371 - val_loss: 1.3595 - val_accuracy: 0.5336\n",
            "Epoch 8/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.0637 - accuracy: 0.6507 - val_loss: 1.0935 - val_accuracy: 0.6504\n",
            "Epoch 9/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 1.0358 - accuracy: 0.6599 - val_loss: 1.0794 - val_accuracy: 0.6425\n",
            "Epoch 10/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 1.0113 - accuracy: 0.6709 - val_loss: 1.0407 - val_accuracy: 0.6531\n",
            "Epoch 11/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.9899 - accuracy: 0.6765 - val_loss: 1.0654 - val_accuracy: 0.6531\n",
            "Epoch 12/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.9607 - accuracy: 0.6865 - val_loss: 0.9860 - val_accuracy: 0.6851\n",
            "Epoch 13/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.9536 - accuracy: 0.6878 - val_loss: 0.9904 - val_accuracy: 0.6718\n",
            "Epoch 14/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.9249 - accuracy: 0.6990 - val_loss: 0.9094 - val_accuracy: 0.7153\n",
            "Epoch 15/200\n",
            "79/79 [==============================] - 5s 65ms/step - loss: 0.9043 - accuracy: 0.7053 - val_loss: 0.8633 - val_accuracy: 0.7283\n",
            "Epoch 16/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.8870 - accuracy: 0.7161 - val_loss: 1.0283 - val_accuracy: 0.6584\n",
            "Epoch 17/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.8789 - accuracy: 0.7153 - val_loss: 0.9266 - val_accuracy: 0.6996\n",
            "Epoch 18/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.8640 - accuracy: 0.7224 - val_loss: 0.8683 - val_accuracy: 0.7290\n",
            "Epoch 19/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.8479 - accuracy: 0.7298 - val_loss: 1.0976 - val_accuracy: 0.6541\n",
            "Epoch 20/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.8264 - accuracy: 0.7385 - val_loss: 0.9897 - val_accuracy: 0.6732\n",
            "Epoch 21/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.8242 - accuracy: 0.7391 - val_loss: 0.8554 - val_accuracy: 0.7324\n",
            "Epoch 22/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.8094 - accuracy: 0.7446 - val_loss: 0.8837 - val_accuracy: 0.7203\n",
            "Epoch 23/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7964 - accuracy: 0.7495 - val_loss: 0.8732 - val_accuracy: 0.7294\n",
            "Epoch 24/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7712 - accuracy: 0.7560 - val_loss: 0.9148 - val_accuracy: 0.7128\n",
            "Epoch 25/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.7737 - accuracy: 0.7559 - val_loss: 0.8420 - val_accuracy: 0.7329\n",
            "Epoch 26/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7642 - accuracy: 0.7594 - val_loss: 0.9010 - val_accuracy: 0.7139\n",
            "Epoch 27/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7510 - accuracy: 0.7635 - val_loss: 0.9177 - val_accuracy: 0.7142\n",
            "Epoch 28/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7333 - accuracy: 0.7726 - val_loss: 0.9678 - val_accuracy: 0.6914\n",
            "Epoch 29/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.7366 - accuracy: 0.7711 - val_loss: 0.8075 - val_accuracy: 0.7488\n",
            "Epoch 30/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7145 - accuracy: 0.7792 - val_loss: 0.9607 - val_accuracy: 0.7119\n",
            "Epoch 31/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.7051 - accuracy: 0.7854 - val_loss: 0.7937 - val_accuracy: 0.7588\n",
            "Epoch 32/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.7041 - accuracy: 0.7835 - val_loss: 0.8296 - val_accuracy: 0.7473\n",
            "Epoch 33/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6989 - accuracy: 0.7865 - val_loss: 0.8551 - val_accuracy: 0.7350\n",
            "Epoch 34/200\n",
            "79/79 [==============================] - 5s 66ms/step - loss: 0.6840 - accuracy: 0.7911 - val_loss: 0.8625 - val_accuracy: 0.7452\n",
            "Epoch 35/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6861 - accuracy: 0.7923 - val_loss: 0.8848 - val_accuracy: 0.7301\n",
            "Epoch 36/200\n",
            "79/79 [==============================] - 5s 66ms/step - loss: 0.6702 - accuracy: 0.7986 - val_loss: 0.9326 - val_accuracy: 0.7217\n",
            "Epoch 37/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6655 - accuracy: 0.8029 - val_loss: 0.8642 - val_accuracy: 0.7418\n",
            "Epoch 38/200\n",
            "79/79 [==============================] - 5s 64ms/step - loss: 0.6737 - accuracy: 0.7959 - val_loss: 0.7907 - val_accuracy: 0.7621\n",
            "Epoch 39/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6568 - accuracy: 0.8046 - val_loss: 0.8296 - val_accuracy: 0.7539\n",
            "Epoch 40/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6524 - accuracy: 0.8057 - val_loss: 0.8603 - val_accuracy: 0.7381\n",
            "Epoch 41/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.6410 - accuracy: 0.8099 - val_loss: 0.8802 - val_accuracy: 0.7445\n",
            "Epoch 42/200\n",
            "79/79 [==============================] - 5s 64ms/step - loss: 0.6404 - accuracy: 0.8112 - val_loss: 0.8189 - val_accuracy: 0.7550\n",
            "Epoch 43/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.6292 - accuracy: 0.8155 - val_loss: 0.8439 - val_accuracy: 0.7503\n",
            "Epoch 44/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6248 - accuracy: 0.8185 - val_loss: 0.9348 - val_accuracy: 0.7191\n",
            "Epoch 45/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6193 - accuracy: 0.8203 - val_loss: 0.8700 - val_accuracy: 0.7363\n",
            "Epoch 46/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6165 - accuracy: 0.8201 - val_loss: 0.7880 - val_accuracy: 0.7671\n",
            "Epoch 47/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6130 - accuracy: 0.8231 - val_loss: 0.7889 - val_accuracy: 0.7659\n",
            "Epoch 48/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6036 - accuracy: 0.8253 - val_loss: 0.9242 - val_accuracy: 0.7265\n",
            "Epoch 49/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.6071 - accuracy: 0.8258 - val_loss: 0.8915 - val_accuracy: 0.7370\n",
            "Epoch 50/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5996 - accuracy: 0.8275 - val_loss: 0.7868 - val_accuracy: 0.7736\n",
            "Epoch 51/200\n",
            "79/79 [==============================] - 5s 65ms/step - loss: 0.5915 - accuracy: 0.8344 - val_loss: 0.8627 - val_accuracy: 0.7533\n",
            "Epoch 52/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5927 - accuracy: 0.8316 - val_loss: 0.7866 - val_accuracy: 0.7751\n",
            "Epoch 53/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5908 - accuracy: 0.8357 - val_loss: 0.8115 - val_accuracy: 0.7655\n",
            "Epoch 54/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5810 - accuracy: 0.8364 - val_loss: 0.8088 - val_accuracy: 0.7715\n",
            "Epoch 55/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5784 - accuracy: 0.8400 - val_loss: 0.8524 - val_accuracy: 0.7523\n",
            "Epoch 56/200\n",
            "79/79 [==============================] - 5s 60ms/step - loss: 0.5859 - accuracy: 0.8357 - val_loss: 0.8360 - val_accuracy: 0.7567\n",
            "Epoch 57/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5754 - accuracy: 0.8397 - val_loss: 0.7887 - val_accuracy: 0.7748\n",
            "Epoch 58/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5639 - accuracy: 0.8448 - val_loss: 0.8656 - val_accuracy: 0.7536\n",
            "Epoch 59/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5746 - accuracy: 0.8414 - val_loss: 0.8482 - val_accuracy: 0.7542\n",
            "Epoch 60/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5669 - accuracy: 0.8459 - val_loss: 0.9100 - val_accuracy: 0.7406\n",
            "Epoch 61/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.5649 - accuracy: 0.8454 - val_loss: 0.7984 - val_accuracy: 0.7716\n",
            "Epoch 62/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5595 - accuracy: 0.8487 - val_loss: 0.8557 - val_accuracy: 0.7582\n",
            "Epoch 63/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5579 - accuracy: 0.8485 - val_loss: 0.8110 - val_accuracy: 0.7715\n",
            "Epoch 64/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5630 - accuracy: 0.8459 - val_loss: 0.7965 - val_accuracy: 0.7759\n",
            "Epoch 65/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5440 - accuracy: 0.8554 - val_loss: 0.8130 - val_accuracy: 0.7675\n",
            "Epoch 66/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5470 - accuracy: 0.8534 - val_loss: 0.8309 - val_accuracy: 0.7638\n",
            "Epoch 67/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5415 - accuracy: 0.8547 - val_loss: 0.8855 - val_accuracy: 0.7459\n",
            "Epoch 68/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5389 - accuracy: 0.8573 - val_loss: 0.8703 - val_accuracy: 0.7580\n",
            "Epoch 69/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5386 - accuracy: 0.8589 - val_loss: 0.8217 - val_accuracy: 0.7749\n",
            "Epoch 70/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5324 - accuracy: 0.8592 - val_loss: 0.7985 - val_accuracy: 0.7713\n",
            "Epoch 71/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5405 - accuracy: 0.8576 - val_loss: 0.8222 - val_accuracy: 0.7714\n",
            "Epoch 72/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5349 - accuracy: 0.8592 - val_loss: 0.8538 - val_accuracy: 0.7621\n",
            "Epoch 73/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5290 - accuracy: 0.8633 - val_loss: 0.8423 - val_accuracy: 0.7685\n",
            "Epoch 74/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5174 - accuracy: 0.8667 - val_loss: 0.7919 - val_accuracy: 0.7800\n",
            "Epoch 75/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5345 - accuracy: 0.8600 - val_loss: 0.8407 - val_accuracy: 0.7615\n",
            "Epoch 76/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5215 - accuracy: 0.8650 - val_loss: 0.9288 - val_accuracy: 0.7386\n",
            "Epoch 77/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5290 - accuracy: 0.8652 - val_loss: 0.8209 - val_accuracy: 0.7738\n",
            "Epoch 78/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5325 - accuracy: 0.8624 - val_loss: 0.8914 - val_accuracy: 0.7565\n",
            "Epoch 79/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5182 - accuracy: 0.8683 - val_loss: 0.8476 - val_accuracy: 0.7645\n",
            "Epoch 80/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5158 - accuracy: 0.8693 - val_loss: 0.7940 - val_accuracy: 0.7838\n",
            "Epoch 81/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5120 - accuracy: 0.8712 - val_loss: 0.8042 - val_accuracy: 0.7814\n",
            "Epoch 82/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5132 - accuracy: 0.8700 - val_loss: 0.8298 - val_accuracy: 0.7750\n",
            "Epoch 83/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5070 - accuracy: 0.8720 - val_loss: 0.8160 - val_accuracy: 0.7753\n",
            "Epoch 84/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5142 - accuracy: 0.8701 - val_loss: 0.8892 - val_accuracy: 0.7499\n",
            "Epoch 85/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5004 - accuracy: 0.8762 - val_loss: 0.8080 - val_accuracy: 0.7780\n",
            "Epoch 86/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5037 - accuracy: 0.8734 - val_loss: 0.7893 - val_accuracy: 0.7831\n",
            "Epoch 87/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5078 - accuracy: 0.8716 - val_loss: 0.8522 - val_accuracy: 0.7685\n",
            "Epoch 88/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5107 - accuracy: 0.8735 - val_loss: 0.8119 - val_accuracy: 0.7772\n",
            "Epoch 89/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4961 - accuracy: 0.8754 - val_loss: 0.8207 - val_accuracy: 0.7705\n",
            "Epoch 90/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4898 - accuracy: 0.8786 - val_loss: 0.8296 - val_accuracy: 0.7739\n",
            "Epoch 91/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4977 - accuracy: 0.8767 - val_loss: 0.8959 - val_accuracy: 0.7581\n",
            "Epoch 92/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4958 - accuracy: 0.8773 - val_loss: 0.8385 - val_accuracy: 0.7719\n",
            "Epoch 93/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4995 - accuracy: 0.8736 - val_loss: 0.8136 - val_accuracy: 0.7822\n",
            "Epoch 94/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4865 - accuracy: 0.8813 - val_loss: 0.9207 - val_accuracy: 0.7528\n",
            "Epoch 95/200\n",
            "79/79 [==============================] - 5s 65ms/step - loss: 0.5029 - accuracy: 0.8745 - val_loss: 0.8319 - val_accuracy: 0.7737\n",
            "Epoch 96/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4987 - accuracy: 0.8754 - val_loss: 0.8752 - val_accuracy: 0.7563\n",
            "Epoch 97/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4912 - accuracy: 0.8802 - val_loss: 0.9028 - val_accuracy: 0.7597\n",
            "Epoch 98/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4897 - accuracy: 0.8817 - val_loss: 0.9076 - val_accuracy: 0.7558\n",
            "Epoch 99/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.5047 - accuracy: 0.8747 - val_loss: 0.8564 - val_accuracy: 0.7699\n",
            "Epoch 100/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.5012 - accuracy: 0.8773 - val_loss: 0.8258 - val_accuracy: 0.7789\n",
            "Epoch 101/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4939 - accuracy: 0.8796 - val_loss: 0.8491 - val_accuracy: 0.7633\n",
            "Epoch 102/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4835 - accuracy: 0.8848 - val_loss: 0.9577 - val_accuracy: 0.7440\n",
            "Epoch 103/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4897 - accuracy: 0.8807 - val_loss: 0.9067 - val_accuracy: 0.7531\n",
            "Epoch 104/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4810 - accuracy: 0.8849 - val_loss: 0.8176 - val_accuracy: 0.7799\n",
            "Epoch 105/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4891 - accuracy: 0.8816 - val_loss: 0.8367 - val_accuracy: 0.7727\n",
            "Epoch 106/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4791 - accuracy: 0.8851 - val_loss: 0.8937 - val_accuracy: 0.7613\n",
            "Epoch 107/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4847 - accuracy: 0.8835 - val_loss: 0.8956 - val_accuracy: 0.7576\n",
            "Epoch 108/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4787 - accuracy: 0.8854 - val_loss: 0.8065 - val_accuracy: 0.7861\n",
            "Epoch 109/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4715 - accuracy: 0.8907 - val_loss: 0.8297 - val_accuracy: 0.7838\n",
            "Epoch 110/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4722 - accuracy: 0.8891 - val_loss: 0.8301 - val_accuracy: 0.7811\n",
            "Epoch 111/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4757 - accuracy: 0.8861 - val_loss: 0.8058 - val_accuracy: 0.7851\n",
            "Epoch 112/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4819 - accuracy: 0.8869 - val_loss: 0.8510 - val_accuracy: 0.7737\n",
            "Epoch 113/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4635 - accuracy: 0.8917 - val_loss: 0.8968 - val_accuracy: 0.7590\n",
            "Epoch 114/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4756 - accuracy: 0.8876 - val_loss: 0.8955 - val_accuracy: 0.7592\n",
            "Epoch 115/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4752 - accuracy: 0.8856 - val_loss: 0.8297 - val_accuracy: 0.7791\n",
            "Epoch 116/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4744 - accuracy: 0.8881 - val_loss: 0.8089 - val_accuracy: 0.7870\n",
            "Epoch 117/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4774 - accuracy: 0.8875 - val_loss: 0.8426 - val_accuracy: 0.7738\n",
            "Epoch 118/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4748 - accuracy: 0.8903 - val_loss: 0.8534 - val_accuracy: 0.7715\n",
            "Epoch 119/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4695 - accuracy: 0.8904 - val_loss: 0.8469 - val_accuracy: 0.7734\n",
            "Epoch 120/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4690 - accuracy: 0.8918 - val_loss: 0.8553 - val_accuracy: 0.7738\n",
            "Epoch 121/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4711 - accuracy: 0.8897 - val_loss: 0.8890 - val_accuracy: 0.7651\n",
            "Epoch 122/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4701 - accuracy: 0.8905 - val_loss: 0.8047 - val_accuracy: 0.7901\n",
            "Epoch 123/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4612 - accuracy: 0.8937 - val_loss: 0.8268 - val_accuracy: 0.7800\n",
            "Epoch 124/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4660 - accuracy: 0.8929 - val_loss: 0.8464 - val_accuracy: 0.7734\n",
            "Epoch 125/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4684 - accuracy: 0.8910 - val_loss: 0.8618 - val_accuracy: 0.7721\n",
            "Epoch 126/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4697 - accuracy: 0.8905 - val_loss: 0.8167 - val_accuracy: 0.7842\n",
            "Epoch 127/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4600 - accuracy: 0.8945 - val_loss: 0.8650 - val_accuracy: 0.7731\n",
            "Epoch 128/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4607 - accuracy: 0.8954 - val_loss: 0.8586 - val_accuracy: 0.7730\n",
            "Epoch 129/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4597 - accuracy: 0.8931 - val_loss: 0.8964 - val_accuracy: 0.7608\n",
            "Epoch 130/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4576 - accuracy: 0.8942 - val_loss: 0.8272 - val_accuracy: 0.7842\n",
            "Epoch 131/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4528 - accuracy: 0.8978 - val_loss: 0.8370 - val_accuracy: 0.7715\n",
            "Epoch 132/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4609 - accuracy: 0.8935 - val_loss: 0.8319 - val_accuracy: 0.7796\n",
            "Epoch 133/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4621 - accuracy: 0.8945 - val_loss: 0.8153 - val_accuracy: 0.7840\n",
            "Epoch 134/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4567 - accuracy: 0.8961 - val_loss: 0.9081 - val_accuracy: 0.7530\n",
            "Epoch 135/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4649 - accuracy: 0.8927 - val_loss: 0.8528 - val_accuracy: 0.7710\n",
            "Epoch 136/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4609 - accuracy: 0.8941 - val_loss: 0.8494 - val_accuracy: 0.7756\n",
            "Epoch 137/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4534 - accuracy: 0.8986 - val_loss: 0.8851 - val_accuracy: 0.7682\n",
            "Epoch 138/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4545 - accuracy: 0.8964 - val_loss: 0.9395 - val_accuracy: 0.7561\n",
            "Epoch 139/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4562 - accuracy: 0.8978 - val_loss: 0.9184 - val_accuracy: 0.7574\n",
            "Epoch 140/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4512 - accuracy: 0.8995 - val_loss: 0.8678 - val_accuracy: 0.7741\n",
            "Epoch 141/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4553 - accuracy: 0.8977 - val_loss: 0.8804 - val_accuracy: 0.7681\n",
            "Epoch 142/200\n",
            "79/79 [==============================] - 5s 61ms/step - loss: 0.4582 - accuracy: 0.8964 - val_loss: 0.8409 - val_accuracy: 0.7830\n",
            "Epoch 143/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4591 - accuracy: 0.8970 - val_loss: 0.8345 - val_accuracy: 0.7784\n",
            "Epoch 144/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4454 - accuracy: 0.9006 - val_loss: 0.8622 - val_accuracy: 0.7756\n",
            "Epoch 145/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4467 - accuracy: 0.8993 - val_loss: 0.8593 - val_accuracy: 0.7764\n",
            "Epoch 146/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4505 - accuracy: 0.8995 - val_loss: 0.8377 - val_accuracy: 0.7834\n",
            "Epoch 147/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4441 - accuracy: 0.9020 - val_loss: 0.8604 - val_accuracy: 0.7760\n",
            "Epoch 148/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4508 - accuracy: 0.8986 - val_loss: 0.8774 - val_accuracy: 0.7700\n",
            "Epoch 149/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4410 - accuracy: 0.9004 - val_loss: 0.8507 - val_accuracy: 0.7767\n",
            "Epoch 150/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4464 - accuracy: 0.9007 - val_loss: 0.8572 - val_accuracy: 0.7684\n",
            "Epoch 151/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4479 - accuracy: 0.9007 - val_loss: 0.8466 - val_accuracy: 0.7778\n",
            "Epoch 152/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4448 - accuracy: 0.9007 - val_loss: 0.9202 - val_accuracy: 0.7553\n",
            "Epoch 153/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4383 - accuracy: 0.9022 - val_loss: 0.8597 - val_accuracy: 0.7781\n",
            "Epoch 154/200\n",
            "79/79 [==============================] - 5s 64ms/step - loss: 0.4524 - accuracy: 0.8996 - val_loss: 0.8418 - val_accuracy: 0.7785\n",
            "Epoch 155/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4445 - accuracy: 0.9013 - val_loss: 0.8674 - val_accuracy: 0.7750\n",
            "Epoch 156/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4463 - accuracy: 0.9032 - val_loss: 0.8575 - val_accuracy: 0.7743\n",
            "Epoch 157/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4497 - accuracy: 0.8983 - val_loss: 0.8321 - val_accuracy: 0.7863\n",
            "Epoch 158/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4479 - accuracy: 0.9007 - val_loss: 0.8236 - val_accuracy: 0.7889\n",
            "Epoch 159/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4533 - accuracy: 0.8983 - val_loss: 0.9085 - val_accuracy: 0.7546\n",
            "Epoch 160/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4435 - accuracy: 0.9019 - val_loss: 0.8972 - val_accuracy: 0.7596\n",
            "Epoch 161/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4368 - accuracy: 0.9035 - val_loss: 0.8555 - val_accuracy: 0.7782\n",
            "Epoch 162/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4414 - accuracy: 0.9028 - val_loss: 0.8642 - val_accuracy: 0.7744\n",
            "Epoch 163/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4306 - accuracy: 0.9069 - val_loss: 0.8467 - val_accuracy: 0.7754\n",
            "Epoch 164/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4401 - accuracy: 0.9034 - val_loss: 0.8388 - val_accuracy: 0.7782\n",
            "Epoch 165/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4535 - accuracy: 0.8978 - val_loss: 0.8643 - val_accuracy: 0.7695\n",
            "Epoch 166/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4402 - accuracy: 0.9050 - val_loss: 0.8488 - val_accuracy: 0.7812\n",
            "Epoch 167/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4421 - accuracy: 0.9015 - val_loss: 0.9300 - val_accuracy: 0.7538\n",
            "Epoch 168/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4471 - accuracy: 0.9026 - val_loss: 0.8493 - val_accuracy: 0.7790\n",
            "Epoch 169/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4398 - accuracy: 0.9042 - val_loss: 0.9217 - val_accuracy: 0.7581\n",
            "Epoch 170/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4359 - accuracy: 0.9047 - val_loss: 0.9359 - val_accuracy: 0.7560\n",
            "Epoch 171/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4294 - accuracy: 0.9064 - val_loss: 0.8568 - val_accuracy: 0.7773\n",
            "Epoch 172/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4335 - accuracy: 0.9054 - val_loss: 0.8437 - val_accuracy: 0.7800\n",
            "Epoch 173/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4327 - accuracy: 0.9068 - val_loss: 0.8327 - val_accuracy: 0.7821\n",
            "Epoch 174/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4429 - accuracy: 0.9034 - val_loss: 0.8327 - val_accuracy: 0.7857\n",
            "Epoch 175/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4337 - accuracy: 0.9056 - val_loss: 0.8583 - val_accuracy: 0.7796\n",
            "Epoch 176/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4313 - accuracy: 0.9080 - val_loss: 0.8365 - val_accuracy: 0.7875\n",
            "Epoch 177/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4423 - accuracy: 0.9021 - val_loss: 0.8486 - val_accuracy: 0.7868\n",
            "Epoch 178/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4434 - accuracy: 0.9017 - val_loss: 0.8847 - val_accuracy: 0.7747\n",
            "Epoch 179/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4395 - accuracy: 0.9043 - val_loss: 0.8584 - val_accuracy: 0.7770\n",
            "Epoch 180/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4328 - accuracy: 0.9068 - val_loss: 0.9504 - val_accuracy: 0.7554\n",
            "Epoch 181/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4349 - accuracy: 0.9043 - val_loss: 0.8952 - val_accuracy: 0.7656\n",
            "Epoch 182/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4395 - accuracy: 0.9049 - val_loss: 0.8377 - val_accuracy: 0.7822\n",
            "Epoch 183/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4425 - accuracy: 0.9041 - val_loss: 0.8902 - val_accuracy: 0.7666\n",
            "Epoch 184/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4351 - accuracy: 0.9045 - val_loss: 0.8798 - val_accuracy: 0.7783\n",
            "Epoch 185/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4335 - accuracy: 0.9076 - val_loss: 0.8722 - val_accuracy: 0.7799\n",
            "Epoch 186/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4375 - accuracy: 0.9041 - val_loss: 0.8456 - val_accuracy: 0.7810\n",
            "Epoch 187/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4306 - accuracy: 0.9063 - val_loss: 0.9518 - val_accuracy: 0.7488\n",
            "Epoch 188/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4187 - accuracy: 0.9110 - val_loss: 0.8343 - val_accuracy: 0.7826\n",
            "Epoch 189/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4297 - accuracy: 0.9084 - val_loss: 0.8727 - val_accuracy: 0.7740\n",
            "Epoch 190/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4333 - accuracy: 0.9058 - val_loss: 0.8320 - val_accuracy: 0.7888\n",
            "Epoch 191/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4218 - accuracy: 0.9085 - val_loss: 0.8593 - val_accuracy: 0.7777\n",
            "Epoch 192/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4280 - accuracy: 0.9082 - val_loss: 0.8460 - val_accuracy: 0.7853\n",
            "Epoch 193/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4393 - accuracy: 0.9031 - val_loss: 0.8560 - val_accuracy: 0.7770\n",
            "Epoch 194/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4332 - accuracy: 0.9057 - val_loss: 0.9362 - val_accuracy: 0.7615\n",
            "Epoch 195/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4179 - accuracy: 0.9110 - val_loss: 0.8390 - val_accuracy: 0.7836\n",
            "Epoch 196/200\n",
            "79/79 [==============================] - 5s 62ms/step - loss: 0.4259 - accuracy: 0.9087 - val_loss: 0.8484 - val_accuracy: 0.7815\n",
            "Epoch 197/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4268 - accuracy: 0.9091 - val_loss: 0.8461 - val_accuracy: 0.7838\n",
            "Epoch 198/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4248 - accuracy: 0.9095 - val_loss: 0.8766 - val_accuracy: 0.7754\n",
            "Epoch 199/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4223 - accuracy: 0.9107 - val_loss: 0.9542 - val_accuracy: 0.7512\n",
            "Epoch 200/200\n",
            "79/79 [==============================] - 5s 63ms/step - loss: 0.4200 - accuracy: 0.9115 - val_loss: 1.0439 - val_accuracy: 0.7415\n",
            "{'loss': [1.909169316291809, 1.4701396226882935, 1.3311398029327393, 1.2396955490112305, 1.181218147277832, 1.1365115642547607, 1.1046712398529053, 1.0636937618255615, 1.0357979536056519, 1.0112701654434204, 0.9898926019668579, 0.9606961011886597, 0.953625500202179, 0.9249085187911987, 0.904321014881134, 0.8870015740394592, 0.8789070248603821, 0.8640457987785339, 0.8479224443435669, 0.8264133930206299, 0.8241863250732422, 0.8093752264976501, 0.7963526844978333, 0.7712307572364807, 0.7737313508987427, 0.7641503214836121, 0.7509854435920715, 0.7333371043205261, 0.736631453037262, 0.7144524455070496, 0.7050631642341614, 0.7040705680847168, 0.6989255547523499, 0.6839697957038879, 0.6861363053321838, 0.6701904535293579, 0.6655468940734863, 0.6737423539161682, 0.6568389534950256, 0.6524083018302917, 0.6410077810287476, 0.6404362916946411, 0.6291705369949341, 0.6247835755348206, 0.6193178296089172, 0.6164722442626953, 0.6130375266075134, 0.6036187410354614, 0.6070753931999207, 0.599642276763916, 0.5915243029594421, 0.592690110206604, 0.5908163785934448, 0.5810108184814453, 0.5783951878547668, 0.5859238505363464, 0.5753664374351501, 0.5638545155525208, 0.5746339559555054, 0.5668520331382751, 0.5648776292800903, 0.5594819188117981, 0.5578669905662537, 0.5629851222038269, 0.5439879298210144, 0.547016978263855, 0.5414889454841614, 0.5389343500137329, 0.5386458039283752, 0.5323640704154968, 0.5405457019805908, 0.5348837375640869, 0.529025673866272, 0.5174434781074524, 0.5344597101211548, 0.5214757323265076, 0.5290142297744751, 0.5325305461883545, 0.5182055234909058, 0.5158409476280212, 0.5120252370834351, 0.5131847858428955, 0.50704026222229, 0.5142216682434082, 0.5004270076751709, 0.5036893486976624, 0.5077537894248962, 0.5106921792030334, 0.4961121678352356, 0.48982149362564087, 0.4976963996887207, 0.49579882621765137, 0.4995073676109314, 0.4865235984325409, 0.5029025673866272, 0.49874427914619446, 0.4912043511867523, 0.4896979033946991, 0.504677414894104, 0.5012024641036987, 0.4938681721687317, 0.48348554968833923, 0.48969945311546326, 0.4810238778591156, 0.48910093307495117, 0.47906526923179626, 0.48471811413764954, 0.4787241220474243, 0.47150352597236633, 0.4722224473953247, 0.47565582394599915, 0.48194339871406555, 0.4635339379310608, 0.4756489396095276, 0.47515901923179626, 0.4743526875972748, 0.4774473011493683, 0.47481778264045715, 0.469539612531662, 0.46895527839660645, 0.4711017608642578, 0.4700748920440674, 0.46123433113098145, 0.46597006916999817, 0.46843990683555603, 0.4696882367134094, 0.45995986461639404, 0.46073582768440247, 0.4597170352935791, 0.45757314562797546, 0.4528365731239319, 0.4609282612800598, 0.46207308769226074, 0.4566597640514374, 0.4649113416671753, 0.4609135389328003, 0.45344027876853943, 0.45453333854675293, 0.4561610817909241, 0.4512237310409546, 0.4552746117115021, 0.4581822156906128, 0.45905497670173645, 0.44537168741226196, 0.44672852754592896, 0.450467586517334, 0.4440629482269287, 0.45079028606414795, 0.44101226329803467, 0.446412593126297, 0.44788575172424316, 0.4448346793651581, 0.43833404779434204, 0.45243775844573975, 0.44451430439949036, 0.4463217258453369, 0.44972264766693115, 0.44792452454566956, 0.45332837104797363, 0.4435066878795624, 0.43681180477142334, 0.4414307177066803, 0.43064892292022705, 0.44013363122940063, 0.45348066091537476, 0.4402293562889099, 0.4420910179615021, 0.44705691933631897, 0.4398244023323059, 0.43591323494911194, 0.4293925166130066, 0.43348053097724915, 0.4326590895652771, 0.44293978810310364, 0.4337291419506073, 0.4313192367553711, 0.4422629773616791, 0.4434148073196411, 0.43946215510368347, 0.4328403174877167, 0.43485942482948303, 0.4395327568054199, 0.4424825608730316, 0.4351133406162262, 0.43347567319869995, 0.4374583065509796, 0.43064379692077637, 0.4186701774597168, 0.4297255873680115, 0.43331536650657654, 0.42183050513267517, 0.4280204176902771, 0.4392929673194885, 0.4331837296485901, 0.417861670255661, 0.4258536994457245, 0.42676085233688354, 0.4247678816318512, 0.42229193449020386, 0.4200233519077301], 'accuracy': [0.38429999351501465, 0.5039499998092651, 0.5532500147819519, 0.5850499868392944, 0.6077250242233276, 0.6224750280380249, 0.6370999813079834, 0.6507250070571899, 0.6599249839782715, 0.6708750128746033, 0.6765249967575073, 0.6865249872207642, 0.6877999901771545, 0.6990249752998352, 0.7052749991416931, 0.7160750031471252, 0.7153000235557556, 0.722350001335144, 0.7298499941825867, 0.7384750247001648, 0.7391499876976013, 0.7445999979972839, 0.7494750022888184, 0.7560250163078308, 0.7559249997138977, 0.7593500018119812, 0.7635499835014343, 0.7726250290870667, 0.7710999846458435, 0.7791500091552734, 0.7853749990463257, 0.7835000157356262, 0.7864999771118164, 0.7911249995231628, 0.7922999858856201, 0.798550009727478, 0.8029000163078308, 0.7959250211715698, 0.8046249747276306, 0.8056750297546387, 0.8098999857902527, 0.8111749887466431, 0.815500020980835, 0.8184750080108643, 0.8202750086784363, 0.8201249837875366, 0.8231499791145325, 0.8253250122070312, 0.8257750272750854, 0.8275250196456909, 0.8344249725341797, 0.8316249847412109, 0.8356500267982483, 0.8364499807357788, 0.8399500250816345, 0.8357499837875366, 0.8396999835968018, 0.8447750210762024, 0.8414000272750854, 0.8458750247955322, 0.8453999757766724, 0.8487250208854675, 0.8485249876976013, 0.8458750247955322, 0.8553749918937683, 0.8533750176429749, 0.8546749949455261, 0.8572999835014343, 0.8588749766349792, 0.8591750264167786, 0.8575999736785889, 0.8592000007629395, 0.863349974155426, 0.8666999936103821, 0.8599749803543091, 0.8649749755859375, 0.8651999831199646, 0.8624250292778015, 0.868274986743927, 0.8692749738693237, 0.8712000250816345, 0.8699749708175659, 0.8720250129699707, 0.8701000213623047, 0.8762249946594238, 0.873449981212616, 0.8715999722480774, 0.8734999895095825, 0.8754249811172485, 0.878600001335144, 0.8766999840736389, 0.8773249983787537, 0.87357497215271, 0.8812749981880188, 0.8744750022888184, 0.8754000067710876, 0.8802000284194946, 0.881725013256073, 0.8747249841690063, 0.8772749900817871, 0.8795750141143799, 0.8848000168800354, 0.8806750178337097, 0.8849499821662903, 0.8816249966621399, 0.8851249814033508, 0.8834999799728394, 0.8853750228881836, 0.890749990940094, 0.889074981212616, 0.8860999941825867, 0.8868749737739563, 0.8917499780654907, 0.8876000046730042, 0.8855999708175659, 0.8881250023841858, 0.8874750137329102, 0.8903250098228455, 0.890375018119812, 0.8918499946594238, 0.8897250294685364, 0.8904500007629395, 0.8937000036239624, 0.8929250240325928, 0.8910250067710876, 0.8904749751091003, 0.8945249915122986, 0.895425021648407, 0.8931249976158142, 0.8941749930381775, 0.897849977016449, 0.8935250043869019, 0.8944500088691711, 0.8961250185966492, 0.8927249908447266, 0.89410001039505, 0.8985999822616577, 0.8963750004768372, 0.897849977016449, 0.8995000123977661, 0.8977000117301941, 0.8963500261306763, 0.8970000147819519, 0.9005500078201294, 0.8992999792098999, 0.8994749784469604, 0.9019749760627747, 0.8986250162124634, 0.9004499912261963, 0.9006749987602234, 0.9006999731063843, 0.9007250070571899, 0.9022499918937683, 0.899649977684021, 0.9012749791145325, 0.903249979019165, 0.8982999920845032, 0.9006999731063843, 0.8982750177383423, 0.9018750190734863, 0.9034500122070312, 0.9028000235557556, 0.9069499969482422, 0.9034000039100647, 0.8977749943733215, 0.9050499796867371, 0.9015499949455261, 0.902649998664856, 0.9042249917984009, 0.9047250151634216, 0.9064249992370605, 0.9053999781608582, 0.9067500233650208, 0.9033750295639038, 0.9056000113487244, 0.9079999923706055, 0.9021499752998352, 0.9016749858856201, 0.904325008392334, 0.9067500233650208, 0.904325008392334, 0.9049249887466431, 0.904075026512146, 0.9045249819755554, 0.9076499938964844, 0.9041249752044678, 0.9063000082969666, 0.9109749794006348, 0.908424973487854, 0.9057750105857849, 0.9085000157356262, 0.908174991607666, 0.903124988079071, 0.9056500196456909, 0.9110000133514404, 0.9086750149726868, 0.9090999960899353, 0.9094750285148621, 0.9106749892234802, 0.9115250110626221], 'val_loss': [2.8837435245513916, 3.3146512508392334, 3.1167352199554443, 3.036001443862915, 2.2792046070098877, 1.9412707090377808, 1.3594619035720825, 1.0934981107711792, 1.07942533493042, 1.0406577587127686, 1.0653817653656006, 0.9859848618507385, 0.9903899431228638, 0.9093775153160095, 0.8632537126541138, 1.0282554626464844, 0.9266458749771118, 0.8682572245597839, 1.0976403951644897, 0.9896616339683533, 0.855361819267273, 0.8836908340454102, 0.8732475638389587, 0.9148353338241577, 0.8419936299324036, 0.9010038375854492, 0.917687714099884, 0.9677659869194031, 0.8075184226036072, 0.9606578350067139, 0.7937418222427368, 0.8296126127243042, 0.8550994396209717, 0.8625472784042358, 0.8848020434379578, 0.932637095451355, 0.8641648292541504, 0.790662944316864, 0.8296148180961609, 0.8602927923202515, 0.8801640868186951, 0.8189147710800171, 0.8438546657562256, 0.9347959160804749, 0.8699743151664734, 0.7879636883735657, 0.7889262437820435, 0.9241676926612854, 0.8915303945541382, 0.786822497844696, 0.8627375960350037, 0.7865511178970337, 0.8115050196647644, 0.8087553977966309, 0.8523937463760376, 0.8359927535057068, 0.7887334227561951, 0.865642786026001, 0.8481518626213074, 0.9100028276443481, 0.7984238862991333, 0.855739176273346, 0.8109696507453918, 0.7965477705001831, 0.8130248188972473, 0.830923318862915, 0.8854790925979614, 0.8703301548957825, 0.8217412829399109, 0.7984743714332581, 0.8221867084503174, 0.8538439273834229, 0.8423125743865967, 0.7918703556060791, 0.8407345414161682, 0.9288209676742554, 0.820945680141449, 0.8914098739624023, 0.8475697040557861, 0.7939601540565491, 0.8042204976081848, 0.8297644257545471, 0.8159506916999817, 0.8892065286636353, 0.8079590201377869, 0.7892757058143616, 0.8522253632545471, 0.811909556388855, 0.8207163214683533, 0.8296418786048889, 0.8958503007888794, 0.8385035395622253, 0.8136178255081177, 0.9206576347351074, 0.8318710923194885, 0.8751698136329651, 0.9028098583221436, 0.9076260924339294, 0.8564410209655762, 0.8258296847343445, 0.8490931391716003, 0.957749605178833, 0.9067319631576538, 0.817592203617096, 0.8366789817810059, 0.8936935663223267, 0.8955694437026978, 0.8065383434295654, 0.8296743035316467, 0.8300927877426147, 0.8058467507362366, 0.8509925603866577, 0.8967751264572144, 0.8954896330833435, 0.8296898603439331, 0.8088508248329163, 0.84262615442276, 0.8533775210380554, 0.8469017744064331, 0.8552621006965637, 0.8889517784118652, 0.8047314882278442, 0.8267744183540344, 0.8464416861534119, 0.8617929816246033, 0.8166953921318054, 0.8650009036064148, 0.8586488962173462, 0.896355152130127, 0.8271574378013611, 0.8369868397712708, 0.8318790793418884, 0.8153476715087891, 0.9081376194953918, 0.8527622818946838, 0.8494234085083008, 0.8850506544113159, 0.9395148158073425, 0.9184339046478271, 0.8677586913108826, 0.8803853392601013, 0.8409417867660522, 0.8345193266868591, 0.8622291088104248, 0.8593143820762634, 0.8377139568328857, 0.8604300022125244, 0.8774027228355408, 0.8506870269775391, 0.857175886631012, 0.846580982208252, 0.9201797842979431, 0.8596704006195068, 0.8417624235153198, 0.8673791289329529, 0.8574740290641785, 0.8321191668510437, 0.8236179947853088, 0.9085462093353271, 0.8972476720809937, 0.8555441498756409, 0.8642282485961914, 0.8467130064964294, 0.8387657403945923, 0.8642749786376953, 0.8487801551818848, 0.9300206899642944, 0.8492844700813293, 0.9217069149017334, 0.935862123966217, 0.8568491339683533, 0.8437032103538513, 0.8327301740646362, 0.8326977491378784, 0.8582695126533508, 0.8364747166633606, 0.8485862016677856, 0.8846769332885742, 0.8584036827087402, 0.9503605365753174, 0.8951758742332458, 0.837720513343811, 0.8901538252830505, 0.8798386454582214, 0.8721998929977417, 0.8455571532249451, 0.9518131613731384, 0.8342621922492981, 0.8727063536643982, 0.8320341110229492, 0.8593059778213501, 0.8459776639938354, 0.8560229539871216, 0.9362047910690308, 0.8390157222747803, 0.8484117388725281, 0.8460865020751953, 0.8766189217567444, 0.9541813731193542, 1.043903112411499], 'val_accuracy': [0.09520000219345093, 0.10279999673366547, 0.10589999705553055, 0.12370000034570694, 0.29600000381469727, 0.36230000853538513, 0.5335999727249146, 0.6503999829292297, 0.6424999833106995, 0.6531000137329102, 0.6531000137329102, 0.6851000189781189, 0.6718000173568726, 0.7153000235557556, 0.7282999753952026, 0.6583999991416931, 0.6995999813079834, 0.7289999723434448, 0.6541000008583069, 0.6732000112533569, 0.7324000000953674, 0.720300018787384, 0.7293999791145325, 0.7128000259399414, 0.7329000234603882, 0.7139000296592712, 0.7142000198364258, 0.6913999915122986, 0.7487999796867371, 0.711899995803833, 0.7588000297546387, 0.7473000288009644, 0.7350000143051147, 0.745199978351593, 0.7300999760627747, 0.7217000126838684, 0.7418000102043152, 0.7620999813079834, 0.7538999915122986, 0.738099992275238, 0.7444999814033508, 0.7549999952316284, 0.7502999901771545, 0.7190999984741211, 0.736299991607666, 0.7670999765396118, 0.7659000158309937, 0.7264999747276306, 0.7369999885559082, 0.7735999822616577, 0.7533000111579895, 0.7750999927520752, 0.765500009059906, 0.7714999914169312, 0.7523000240325928, 0.7566999793052673, 0.7748000025749207, 0.753600001335144, 0.7541999816894531, 0.7405999898910522, 0.7716000080108643, 0.7581999897956848, 0.7714999914169312, 0.7759000062942505, 0.7674999833106995, 0.7638000249862671, 0.7458999752998352, 0.7580000162124634, 0.7749000191688538, 0.7713000178337097, 0.771399974822998, 0.7620999813079834, 0.7684999704360962, 0.7799999713897705, 0.7615000009536743, 0.7386000156402588, 0.7738000154495239, 0.7565000057220459, 0.7645000219345093, 0.7838000059127808, 0.7814000248908997, 0.7749999761581421, 0.7753000259399414, 0.7498999834060669, 0.777999997138977, 0.7831000089645386, 0.7684999704360962, 0.7771999835968018, 0.7705000042915344, 0.7738999724388123, 0.7580999732017517, 0.7718999981880188, 0.7821999788284302, 0.7527999877929688, 0.7736999988555908, 0.7562999725341797, 0.7597000002861023, 0.7558000087738037, 0.7699000239372253, 0.7789000272750854, 0.7633000016212463, 0.7440000176429749, 0.7530999779701233, 0.7799000144004822, 0.7727000117301941, 0.7613000273704529, 0.7576000094413757, 0.7860999703407288, 0.7838000059127808, 0.7810999751091003, 0.785099983215332, 0.7736999988555908, 0.7590000033378601, 0.7591999769210815, 0.7791000008583069, 0.7870000004768372, 0.7738000154495239, 0.7714999914169312, 0.7734000086784363, 0.7738000154495239, 0.7651000022888184, 0.7900999784469604, 0.7799999713897705, 0.7734000086784363, 0.7720999717712402, 0.7842000126838684, 0.7731000185012817, 0.7730000019073486, 0.7608000040054321, 0.7842000126838684, 0.7714999914169312, 0.7796000242233276, 0.7839999794960022, 0.753000020980835, 0.7710000276565552, 0.775600016117096, 0.7681999802589417, 0.7560999989509583, 0.7573999762535095, 0.7741000056266785, 0.7681000232696533, 0.7829999923706055, 0.7784000039100647, 0.775600016117096, 0.7764000296592712, 0.7833999991416931, 0.7760000228881836, 0.7699999809265137, 0.7767000198364258, 0.7684000134468079, 0.7778000235557556, 0.755299985408783, 0.7781000137329102, 0.7785000205039978, 0.7749999761581421, 0.7742999792098999, 0.786300003528595, 0.7889000177383423, 0.7545999884605408, 0.7595999836921692, 0.7781999707221985, 0.774399995803833, 0.7753999829292297, 0.7781999707221985, 0.7695000171661377, 0.7811999917030334, 0.7537999749183655, 0.7789999842643738, 0.7580999732017517, 0.7559999823570251, 0.7773000001907349, 0.7799999713897705, 0.7821000218391418, 0.7857000231742859, 0.7796000242233276, 0.7875000238418579, 0.7868000268936157, 0.7746999859809875, 0.7770000100135803, 0.7554000020027161, 0.7656000256538391, 0.7821999788284302, 0.7666000127792358, 0.7782999873161316, 0.7799000144004822, 0.781000018119812, 0.7487999796867371, 0.7825999855995178, 0.7739999890327454, 0.7888000011444092, 0.7777000069618225, 0.7853000164031982, 0.7770000100135803, 0.7615000009536743, 0.7835999727249146, 0.781499981880188, 0.7838000059127808, 0.7753999829292297, 0.7512000203132629, 0.7415000200271606]}\n"
          ]
        }
      ],
      "source": [
        "#Train the Model\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        " filepath='/tmp/checkpoint',\n",
        " save_weights_only=True,\n",
        " monitor = 'val_accuracy',\n",
        " mode='max',\n",
        " save_best_only=True)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 200,\n",
        "                     batch_size=512,shuffle=True,\n",
        "                     validation_split=0.2,callbacks=[model_checkpoint_callback])\n",
        "print(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "nN2z77iAkpft",
        "outputId": "c449bbd7-5f52-4721-d0cc-90db3442ffc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.3604948818683624, 0.9487599730491638]\n",
            "[0.814699649810791, 0.7836999893188477]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the best weights\n",
        "model.load_weights('/tmp/checkpoint')\n",
        "\n",
        "print(model.evaluate(x_train, y_train, verbose=0))\n",
        "print(model.evaluate(x_test, y_test, verbose=0))\n",
        "\n",
        "# Plot the training loss and validation loss\n",
        "loss_train = np.array(history.history['loss'])\n",
        "loss_test = np.array(history.history['val_loss'])\n",
        "\n",
        "x = np.arange(0, loss_train.shape[0])\n",
        "plt.plot(x, loss_train, label=\"Training loss\")\n",
        "plt.plot(x, loss_test, label=\"Validation loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Css324f21_hw_ml_Pram&KT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
